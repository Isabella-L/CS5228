{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c277971f-3186-4675-b547-41e26fa436d2",
   "metadata": {},
   "source": [
    "<img src=\"images/cs5228-header-title.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a046cab-f59e-4343-9824-20d91b0d53dd",
   "metadata": {},
   "source": [
    "# Assignment 1 - Exploratory Data Analysis (EDA) & Clustering\n",
    "\n",
    "Hello everyone, this assignment notebook covers Exploratory Data Analysis (EDA) and Clustering. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Remember to rename and save this Jupyter notebook as **A1_YourName_YourNUSNETID.ipynb** (e.g., **A1_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Remember to rename and save the Python script file **A1_YourName_YourNUSNETID.py** (e.g., **A1_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is **Sep 18, 11.59 pm**. Late submissions will be penalized by 10% for each additional day. There is no need to use your full name if it's rather long; it's just important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c8a8d7-f815-4bfe-b9bf-822d0ae16734",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 'A0223593A'\n",
    "nusnet_id = 'E0564634'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982a73c-a2c1-42d5-84e7-dc8444038f2f",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well documented, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Exploratory Data Analysis (EDA) & Data Preparation (25 Points)**\n",
    "    * 1.1 Removing \"Dirty\" Records (5 Points)\n",
    "    * 1.2 Handling Missing (NaN) Values (5 Points)\n",
    "    * 1.3 Other Appropriate Data Cleaning / Preprocessing Steps (5 Points)\n",
    "        * 1.3 a) Appropriate Steps (3 Points)\n",
    "        * 1.3 b) Additional Considerations (2 Points)\n",
    "    * 1.4 Handling Categorical Attributes (4 Points)\n",
    "    * 1.5 Basic Facts about a Real-World Dataset (6 Points)\n",
    "* **2 Clustering (25 Points)**\n",
    "    * 2.1 Implementing DBSCAN for Noise Detection (8 Points)\n",
    "        * 2.1 a) Compute Core Points (4 Points)\n",
    "        * 2.1 b) Compute Noise Points (4 Points)\n",
    "    * 2.2 Questions about Clustering Algorithms (17 Points)\n",
    "        * 2.2 a) Interpreting Dendrograms for Hierarchical Clusterings (6 Points)\n",
    "        * 2.2 b) Comparing the Results of Different Clustering Algorithms (6 Points)\n",
    "        * 2.2 c) Short Essay Questions (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77b3f1-9e57-4068-bd13-9940aa87f3ec",
   "metadata": {},
   "source": [
    "### Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48147e2c-c1e7-4aa7-93b4-3e08c25286ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some magic so that the notebook will reload the external python script file any time you edit and save the .py file;\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f1bff-bed3-47da-bb69-51fec4a1576e",
   "metadata": {},
   "source": [
    "Making all the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ecf794-d9f9-43b1-a604-6616af7e06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20afcf-b880-4dda-b1bd-81e28a2df97a",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `A1.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d639429-aa27-4c1e-87fe-9eac5bb41fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from A1 import *\n",
    "#from A1_BobSmith_e12345678 import * # <-- you well need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7e9bd-727a-4d38-8c18-033ddb030f23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da13663-dec8-4634-9d5e-a4d4fcf6c1d5",
   "metadata": {},
   "source": [
    "# 1 Exploratory Data Analysis (EDA) & Data Preparation (25 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc15f2-6d3f-4a2a-91cd-05bdd50f3432",
   "metadata": {},
   "source": [
    "### 1.1 Removing \"Dirty\" Records (5 Points)\n",
    "\n",
    "Assume that you have been tasked to build a regression model to predict the **resale prices of used cars** in Singapore. To this end, you get a dataset containing information about 15,000 past resale transactions, including the following information about the attributes:\n",
    "\n",
    "* **listing_id**: Unique ID of the listing; an integer number uniquely assigned to each listing. If this code starts with the letter 'C', it indicates a cancellation of the listing and this is an invalid listing.\n",
    "* **url**: URL of the website where the listing is posted as string value\n",
    "* **make**: The make/brand of the car as a string value of a valid car make (e.g., \"bmw\", \"honda\", \"mazda\", \"toyota\", \"mercedes-benz\").\n",
    "* **model**: The model of the car as a string value of a valid car model (e.g., \"e250\", \"outlander\", \"v60\", \"x1\", \"qashqai\").\n",
    "* **manufactured**: The year the car has been manufactured as an integer value.\n",
    "* **type_of_vehicle**: The type of the car as a string value of a valid vehicle type (e.g., \"luxury sedan\", \"mid-sized sedan\", \"suv\", \"hatchback\").\n",
    "* **power**: The power of the engine as an integer value in kW (kilowatt)\n",
    "* **engine_cap**: Size/capacity of the engine as integer value in cc (cubic centimeter)\n",
    "* **curb_weight**: the weight of the vehicle including a full tank of fuel and all standard equipment in kg (kilogram)\n",
    "* **no_of_owners**: Number of previous owners as integer value >= 1 (used cars only).\n",
    "* **mileage**: Mileage of car as integer value in km (kilometer)\n",
    "* **price**: Resale price of the car in Singapore dollars.\n",
    "\n",
    "Let's have a first look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6424d61d-2811-48ff-a33b-8ec613593802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>url</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>manufactured</th>\n",
       "      <th>type_of_vehicle</th>\n",
       "      <th>power</th>\n",
       "      <th>engine_cap</th>\n",
       "      <th>curb_weight</th>\n",
       "      <th>no_of_owners</th>\n",
       "      <th>mileage</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>983884</td>\n",
       "      <td>https://www.sgcarmart.com/listing/983884</td>\n",
       "      <td>nissan</td>\n",
       "      <td>qashqai</td>\n",
       "      <td>2015</td>\n",
       "      <td>suv</td>\n",
       "      <td>85</td>\n",
       "      <td>1197</td>\n",
       "      <td>1285</td>\n",
       "      <td>2</td>\n",
       "      <td>80000</td>\n",
       "      <td>52800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1026259</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1026259</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>golf</td>\n",
       "      <td>2010</td>\n",
       "      <td>sports car</td>\n",
       "      <td>188</td>\n",
       "      <td>1984</td>\n",
       "      <td>1466</td>\n",
       "      <td>6</td>\n",
       "      <td>149000</td>\n",
       "      <td>105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002033</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1002033</td>\n",
       "      <td>suzuki</td>\n",
       "      <td>vitara</td>\n",
       "      <td>2016</td>\n",
       "      <td>suv</td>\n",
       "      <td>88</td>\n",
       "      <td>1586</td>\n",
       "      <td>1160</td>\n",
       "      <td>1</td>\n",
       "      <td>84000</td>\n",
       "      <td>55800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1030109</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1030109</td>\n",
       "      <td>kia</td>\n",
       "      <td>picanto</td>\n",
       "      <td>2007</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>47</td>\n",
       "      <td>1086</td>\n",
       "      <td>852</td>\n",
       "      <td>2</td>\n",
       "      <td>105000</td>\n",
       "      <td>8888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1013252</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1013252</td>\n",
       "      <td>porsche</td>\n",
       "      <td>cayenne</td>\n",
       "      <td>2014</td>\n",
       "      <td>suv</td>\n",
       "      <td>193</td>\n",
       "      <td>2967</td>\n",
       "      <td>2185</td>\n",
       "      <td>3</td>\n",
       "      <td>108650</td>\n",
       "      <td>162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1029331</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1029331</td>\n",
       "      <td>bmw</td>\n",
       "      <td>530i</td>\n",
       "      <td>2020</td>\n",
       "      <td>luxury sedan</td>\n",
       "      <td>187</td>\n",
       "      <td>1998</td>\n",
       "      <td>1625</td>\n",
       "      <td>1</td>\n",
       "      <td>6800</td>\n",
       "      <td>273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1006727</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1006727</td>\n",
       "      <td>volkswagen</td>\n",
       "      <td>sharan</td>\n",
       "      <td>2017</td>\n",
       "      <td>mpv</td>\n",
       "      <td>162</td>\n",
       "      <td>1984</td>\n",
       "      <td>1790</td>\n",
       "      <td>2</td>\n",
       "      <td>71000</td>\n",
       "      <td>101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1025136</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1025136</td>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>e200</td>\n",
       "      <td>2017</td>\n",
       "      <td>luxury sedan</td>\n",
       "      <td>135</td>\n",
       "      <td>1991</td>\n",
       "      <td>1605</td>\n",
       "      <td>1</td>\n",
       "      <td>65000</td>\n",
       "      <td>149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1012988</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1012988</td>\n",
       "      <td>toyota</td>\n",
       "      <td>corolla</td>\n",
       "      <td>2009</td>\n",
       "      <td>mid-sized sedan</td>\n",
       "      <td>80</td>\n",
       "      <td>1598</td>\n",
       "      <td>1195</td>\n",
       "      <td>2</td>\n",
       "      <td>113000</td>\n",
       "      <td>49800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1016138</td>\n",
       "      <td>https://www.sgcarmart.com/listing/1016138</td>\n",
       "      <td>bmw</td>\n",
       "      <td>520d</td>\n",
       "      <td>2013</td>\n",
       "      <td>luxury sedan</td>\n",
       "      <td>135</td>\n",
       "      <td>1995</td>\n",
       "      <td>1700</td>\n",
       "      <td>1</td>\n",
       "      <td>119000</td>\n",
       "      <td>62000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id                                        url           make  \\\n",
       "0      983884   https://www.sgcarmart.com/listing/983884         nissan   \n",
       "1     1026259  https://www.sgcarmart.com/listing/1026259     volkswagen   \n",
       "2     1002033  https://www.sgcarmart.com/listing/1002033         suzuki   \n",
       "3     1030109  https://www.sgcarmart.com/listing/1030109            kia   \n",
       "4     1013252  https://www.sgcarmart.com/listing/1013252        porsche   \n",
       "..        ...                                        ...            ...   \n",
       "95    1029331  https://www.sgcarmart.com/listing/1029331            bmw   \n",
       "96    1006727  https://www.sgcarmart.com/listing/1006727     volkswagen   \n",
       "97    1025136  https://www.sgcarmart.com/listing/1025136  mercedes-benz   \n",
       "98    1012988  https://www.sgcarmart.com/listing/1012988         toyota   \n",
       "99    1016138  https://www.sgcarmart.com/listing/1016138            bmw   \n",
       "\n",
       "      model  manufactured  type_of_vehicle  power  engine_cap curb_weight  \\\n",
       "0   qashqai          2015              suv     85        1197        1285   \n",
       "1      golf          2010       sports car    188        1984        1466   \n",
       "2    vitara          2016              suv     88        1586        1160   \n",
       "3   picanto          2007        hatchback     47        1086         852   \n",
       "4   cayenne          2014              suv    193        2967        2185   \n",
       "..      ...           ...              ...    ...         ...         ...   \n",
       "95     530i          2020     luxury sedan    187        1998        1625   \n",
       "96   sharan          2017              mpv    162        1984        1790   \n",
       "97     e200          2017     luxury sedan    135        1991        1605   \n",
       "98  corolla          2009  mid-sized sedan     80        1598        1195   \n",
       "99     520d          2013     luxury sedan    135        1995        1700   \n",
       "\n",
       "    no_of_owners  mileage   price  \n",
       "0              2    80000   52800  \n",
       "1              6   149000  105800  \n",
       "2              1    84000   55800  \n",
       "3              2   105000    8888  \n",
       "4              3   108650  162800  \n",
       "..           ...      ...     ...  \n",
       "95             1     6800  273800  \n",
       "96             2    71000  101800  \n",
       "97             1    65000  149800  \n",
       "98             2   113000   49800  \n",
       "99             1   119000   62000  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cars_dirty = pd.read_csv('data/a1-used-cars-resale-dirty.csv') # panda dataframe\n",
    "\n",
    "df_cars_dirty.head(100) # shows first 5 rows  # summary of dataframe, including number of non-null entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f6a9fd-3be5-4d2d-836b-4a3473f24883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 15000\n"
     ]
    }
   ],
   "source": [
    "print('Number of records: {}'.format(len(df_cars_dirty)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff12ce5-5661-4e5f-8561-29c0fda07582",
   "metadata": {},
   "source": [
    "If you check the dataset against its description as given above -- with the help of `pandas` or by simply inspecting the raw data file -- you will notice that many records are \"dirty\", meaning they are not in the expected format. Dirty records can negatively affect any subsequent analysis it needs.\n",
    "\n",
    "**Perform EDA on the Used Cars Resale Price dataset and perform appropriate preprocessing steps to clean the data!**\n",
    "The preprocessing step for cleaning the data step may include\n",
    "* the *removal* of \"dirty records\" or\n",
    "* the *modification* of \"dirty records\" records\n",
    "\n",
    "**Task 1: Identify at least 3 issues** with the dataset that would negatively affect any subsequent analysis, and clean the data accordingly.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "* Recall from the lecture that data cleaning often involves making certain decisions. As such, you might come up with different steps than other students. This is OK as long as you can reasonably justify your steps.\n",
    "* The goal is to preserve as much of the records as possible! So only remove records as part of your data cleaning if it's really necessary (this includes that you should not remove any attributes!). There might be different valid cases, so don't forget to briefly justify your decision.\n",
    "* For this task, we are **not** trying to identify \"noise\" (e.g., outliers in the form of suspicious values) or `NaN` values; this will be covered later. Here, we look for records with values not eligible to be used for an analysis because they do not adhere to the data description.\n",
    "\n",
    "Please provide your answer below. It should list the different issues you have identified and briefly discuss which data cleaning steps you can and/or need to perform to address those issues.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7cbe52-8263-49ac-927f-4673d4ff722c",
   "metadata": {},
   "source": [
    "1. Having cancelled listing are problematic for the analysis of resale prices in singapore since they do not represent actual transactions, keeping them can lead to bias results. Although they might be useful in times when studying market dynamics, the records -- which are invalid by definition -- are not helpful in the current use case. Hence, I purges all the entries with `listing_id` starting with `C`\n",
    "2. Some entries in the `type_of_vehicle` attribute are `unknown`. I inspected the nomical attributes `make` and `type_of_vehicle` of the dataset using pandas `unique()` function and found out there are some unidentified types in vehicles marked as `unknown`. Instead of removing the entries, I replaced `unknown` with `NaN` to make them as missing without discarding other potentially useful information such as brand, mileage and price. \n",
    "3. Numeric fields `manufactured` and `no_of_owners` contained values outside their valid ranges. The manufactured year must not exceed the current year (2025), while no_of_owners must be at least 1 according to the dataset definition. Since there is no additional information to help correct those information, these entries are seen as invalid and removed all together. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid listing ids: 453\n",
      "19       C1015041\n",
      "21       C1010948\n",
      "39       C1024902\n",
      "163      C1019213\n",
      "171       C988905\n",
      "           ...   \n",
      "14884     C965918\n",
      "14902    C1019527\n",
      "14924    C1027433\n",
      "14931    C1024865\n",
      "14951    C1021111\n",
      "Name: listing_id, Length: 453, dtype: object\n",
      "Number of invalid manufactured year: 306\n",
      "90       2116\n",
      "163      2118\n",
      "169      2116\n",
      "188      2115\n",
      "199      2120\n",
      "         ... \n",
      "14838    2119\n",
      "14891    2111\n",
      "14913    2117\n",
      "14916    2117\n",
      "14924    2116\n",
      "Name: manufactured, Length: 306, dtype: int64\n",
      "Numer of invalid owner: 455\n",
      "12      -1\n",
      "53      -1\n",
      "102     -1\n",
      "113     -1\n",
      "140     -1\n",
      "        ..\n",
      "14820   -1\n",
      "14852   -1\n",
      "14866   -1\n",
      "14890   -1\n",
      "14997   -1\n",
      "Name: no_of_owners, Length: 455, dtype: int64\n",
      "['nissan' 'volkswagen' 'suzuki' 'kia' 'porsche' 'toyota' 'bmw'\n",
      " 'mercedes-benz' 'mitsubishi' 'skoda' 'volvo' 'honda' 'mclaren' 'ferrari'\n",
      " 'lexus' 'austin' 'hyundai' 'mazda' 'audi' 'mini' 'infiniti' 'lamborghini'\n",
      " 'daihatsu' 'jaguar' 'subaru' 'chevrolet' 'maserati' 'ford' 'alfa romeo'\n",
      " 'peugeot' 'bentley' 'opel' 'land rover' 'seat' 'rolls-royce' 'ssangyong'\n",
      " 'renault' 'fiat' 'citroen' 'aston martin' 'hummer' 'jeep' 'mercedes'\n",
      " 'proton' 'mg' 'cupra' 'lotus' 'dodge' 'daimler' 'mitsuoka' 'perodua'\n",
      " 'cadillac' 'ruf' 'rolls' 'maybach' 'morgan' 'chrysler' 'alpine' 'smart'\n",
      " 'saab' 'dongfeng' 'maxus' 'international']\n",
      "['suv' 'sports car' 'hatchback' 'mid-sized sedan' 'luxury sedan' 'mpv'\n",
      " 'unknown' 'stationwagon' 'bus/mini bus' 'van']\n"
     ]
    }
   ],
   "source": [
    "# Convert to string just in case some values are mixed types\n",
    "\n",
    "# listing id start with \"C\" are cancelled \n",
    "invalid_ids = df_cars_dirty[~df_cars_dirty['listing_id'].astype(str).str.isdigit()]\n",
    "print('Number of invalid listing ids: {}'.format(len(invalid_ids)))\n",
    "\n",
    "invalid_year = df_cars_dirty[(df_cars_dirty['manufactured'] < 1900) | (df_cars_dirty['manufactured'] > 2025)]\n",
    "print('Number of invalid manufactured year: {}'.format(len(invalid_year)))\n",
    "\n",
    "invalid_owner = df_cars_dirty[df_cars_dirty['no_of_owners'] < 1]\n",
    "print(\"Numer of invalid owner: {}\".format(len(invalid_owner)))\n",
    "\n",
    "unknown_type = df_cars_dirty[df_cars_dirty['type_of_vehicle'] == 'unknown']\n",
    "print('Number of unknown vehicle type: {}'.format(len(unknown_type)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2224d35-d0c5-48b9-8b6e-56261b2ee96b",
   "metadata": {},
   "source": [
    "**Task 2: Complete the method `clean` in the `.py` file to actually implement your steps for handling \"dirty\" records!** The results should back up your answer above. Hint: Try to use methods/functionalities provided by `pandas` as much as possible; it will make your code much shorter, faster and your life easier. We will run performance checks for this method to evaluate the runtime, but the performance is not considered for the grading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09662556-97c0-4ada-8db1-9e2a5f1058cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_cleaned = clean(df_cars_dirty)\n",
    "\n",
    "print('After preprocessing, there are now {} records.'.format(df_cars_cleaned.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc7308-9b0b-46d3-b55d-b3313c079b10",
   "metadata": {},
   "source": [
    "### 1.2 Handling Missing (NaN) Values (5 Points)\n",
    "\n",
    "Many traditional data mining algorithms do not like missing (NaN) values and will throw an error if missing values are present. We therefore have to address missing values and get rid of them. On the other hand, we want to preserve as much of our dataset as possible, so we need to be smart about that. In this subtask, you are provided with a version of our used cars resale dataset that contains missing values but is otherwise clean -- so it is all about the `NaN` values here.\n",
    "\n",
    "Let's load the dataset and have a quick look -- the attributes are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221af02-de76-48eb-aac8-8ff9f808da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_nan = pd.read_csv('data/a1-used-cars-resale-nan.csv')\n",
    "\n",
    "df_cars_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e998f-4208-4617-9a98-2586459fd01e",
   "metadata": {},
   "source": [
    "Since your decision for handling `NaN` values might depend in the data mining task, assume in the following that you want to use this dataset to **create a regression model to predict the resale price** from the attributes of a transaction. Of course, there will be no need to actually create such a model here.\n",
    "\n",
    "**Task 1: Identify all `NaN` values in the dataset and handle them appropriately!** After this preprocessing, the resulting dataset should no longer contain any `NaN` values. Please provide your answer in the markdown cell below list all issues concerning `NaN` values and how you would handle them with a brief justification for your approach. Additional (simplifying) guidelines:\n",
    "\n",
    "* The goal is to preserve as many records in the dataset as possible.\n",
    "* You do not need to consider external knowledge (i.e., information coming from outside this dataset)\n",
    "* Ignore more sophisticated solutions such as [`sklearn.impute.KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html). These can be very useful in practice (and maybe for your project), but their application requires certain assumptions to hold for good results. This is beyond the scope of this assignment.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5af8e-d452-4f8b-a041-0b500eb674e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7d88a41-64d1-48bc-aa93-dd8a788e762f",
   "metadata": {},
   "source": [
    "**Task 2: Complete the method `handle_nan` in the `.py` file to actually implement your steps for handling `NaN` values!** The results should back up your answer above. Hint: Try to use methods/functionalities provided by `pandas` as much as possible; it will make your code much shorter, faster and your life easier. We will run performance checks for this method to evaluate the runtime, but the performance is not considered for the grading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea3c1a-cce6-4cc6-99ea-7f8177fe35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_no_nan = handle_nan(df_cars_nan)\n",
    "\n",
    "print('After handling missing values, there are now {} records.'.format(df_cars_no_nan.shape[0]))\n",
    "print('Number of records with an NaN for any attribute: {}'.format((df_cars_no_nan.isna().sum(axis=1) > 0).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc33af-e08d-42ae-8923-695da6a8a8ce",
   "metadata": {},
   "source": [
    "### 1.3 Other Appropriate Data Cleaning / Preprocessing Steps (5 Points)\n",
    "\n",
    "Identifying \"dirty\" records and missing data are two very fundamental and generally rather systematic steps as part of data cleaning / data preprocessing. However, as we saw in the lecture using some examples, there are many other issues with the dataset that can be considered noise and thus potentially negatively affecting any data analysis. So the more noise we can remove, the more likely we can expect meaning analysis results.\n",
    "\n",
    "For this subtask, we use a version of our Used Cars Resale dataset **with no \"dirty\" records or missing data**! Note that this dataset has a few more attributes: `eco_category`, `transmission`, and `reg_date` (registration date). Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9589caf9-0b91-4e17-b9af-490478a8986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_others = pd.read_csv('data/a1-used-cars-resale-other.csv')\n",
    "\n",
    "df_cars_others.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9e86d-b045-489e-95f0-72da346a058e",
   "metadata": {},
   "source": [
    "#### 1.3 a) Appropriate Steps (3 Points)\n",
    "\n",
    "**List at least 4 data cleaning / data preprocessing steps you deem appropriate to apply to the dataset above!** Please provide your answer in the markdown cell below list all steps together with a justification for your decision. Additional (simplifying) guidelines:\n",
    "\n",
    "* You should still assume that we want to use this dataset to create a model for predicting the resale price of a flat based on its attributes. The choice of data mining task is very likely to affect your decision for what cleaning / preprocessing steps to apply.\n",
    "* There is no need to consider external knowledge. For example, you do not have to check if a value for `model` is indeed an existing car model.\n",
    "* Please do not address the concept of attribute encoding -- that is, the encoding of categorical attributes as numerical attributes -- in this subtask. This comes later.\n",
    "* There is no need for you to implement any processing steps! Most important are your justifications for your decisions.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce4165-8cd8-49b9-a7ae-8d14249087e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4518b190-2953-4013-8955-5690b8c53909",
   "metadata": {},
   "source": [
    "#### 1.3 b) Additional Considerations (2 Points)\n",
    "\n",
    "So far, we handled \"dirty\" records, missing values, and other types of (arguably) straightforward EDA/preprocessing steps. This means that we performed important steps towards ensuring a minimum level of data quality to, in turn, enable a successful data analysis and meaningful results. However, this does not mean that our dataset is now free of any noise.\n",
    "\n",
    "**Briefly discuss what other kind of noise our dataset might still contain?** Include a brief explanation if and how we could identify and handle these instances of noisy data -- if possible in a meaningful way. There is no need to analyze and check the dataset to find any examples. This question is mainly to motivate some additional thinking.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed8d8f-a64b-447e-9966-23b46d2a0e96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3824507-bc83-4a86-804f-f74d9e40d97b",
   "metadata": {},
   "source": [
    "### 1.4 Handling Categorical Attributes (4 Points)\n",
    "\n",
    "Many to most data mining algorithms require all input features / attributes to be numerical. Our dataset with transactions resales of condo flats contains attributes that are not all numerical. As such, assuming we indeed want to utilize them, we need to convert those attributes into numerical ones. Regarding encoding techniques, we covered [One-Hot Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) in the lecture, and also talked about [Target Encoding](https://contrib.scikit-learn.org/category_encoders/targetencoder.html) in the tutorial; you can and should also look into other encoding techniques.\n",
    "\n",
    "For this task, we assume the same dataset file used in 1.3 (`a1-used-cars-resale-others.csv`).\n",
    "\n",
    "**Briefly(!) discuss how you would handle each categorical attribute and justify why!** Handling a categorical attribute means\n",
    "* to drop a categorical attribute *or*\n",
    "* to encode it into a numerical representation *or*\n",
    "* to otherwise transform it into a numerical attribute\n",
    "\n",
    "There is no single correct answer for this task; it's your justification that matters. Again, assume that we want to create a regression model to predict the resale price of a flat based on the other features.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47758ccc-8179-4cae-90d5-c15fc94b691c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13707a30-7875-4fa1-8222-38b209cc177f",
   "metadata": {},
   "source": [
    "### 1.5 Basic Facts about a Real-World Dataset (6 Points)\n",
    "\n",
    "The following tasks are about getting basic insights into the Used Cars Resale Prices dataset. As the data preprocessing steps you choose to perform might affect the results of this task, we will use a different dataset file here. Note that this file also contains 15,000 listings of used cars but does **not** contain any \"dirty\" records. This is to ensure that everyone uses the same data.\n",
    "\n",
    "If you think this data file may contain \"dirty\" records, you can safely ignore them. Please do not modify the data for this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d208b13-9210-4e37-8639-e787b08872f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_facts = pd.read_csv('data/a1-used-cars-resale-facts.csv')\n",
    "\n",
    "df_cars_facts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e021b4e-7ce3-49e6-9869-0be8c3b0a8c4",
   "metadata": {},
   "source": [
    "**Please complete the table below by answering the 8 given questions!** Use the code cell below the table to actually implement your steps that enabled you to answer the questions. There is no need for a fancy layout for any print statement; it's only important that the result is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e66bb-d5c1-4bf0-a0df-61700e9ba1f6",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(6).\n",
    "\n",
    "| No. | Question                                                                                               \t| Answer   \t|\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------|\n",
    "| 1)  | How many Toyota Corolla (manufactured before 2010) have been sold?  | ??? |\n",
    "| 2)  | What are the top-3 most sold car makes (give the car make and the number of sales)? | ??? |\n",
    "| 3)  | Which SUV car model has been sold the most (give the model and the number of sales)? | ??? |\n",
    "| 4)  | Which car make generated the highest overall sale when only considering low-powered cars, i.e., with power $ \\leq$ 60 (give the car make and the total sale)? | ??? |\n",
    "| 5)  | Which midsize sedan has the highest *power-to-engine_cap* ratio (give, the make, model, year of manufacturing, and power-to-engine_cap ratio (2 decimal precision))?                                                    \t| ??? |\n",
    "| 6)  | What is the correlation between the resale *price* and *mileage*, and between resale *price* and *engine_cap*? Use the Pearson correlation as metric. | ??? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31626bb4-e099-40c1-a791-1993a81c8fa2",
   "metadata": {},
   "source": [
    "**Complete the method `extract_facts` in the `.py` file to actually extract the facts!** The results should back up your answer above. Hint: Try to use methods/functionalities provided by `pandas` as much as possible; it will make your code much shorter, faster and your life easier. We will run performance checks for this method to evaluate the runtime, but the performance is not considered for the grading!\n",
    "\n",
    "**Note:** You can simply use simple print statements that somehow show the result you entered into the table above. You do not have to ensure any specific output. It should only *somehow* be possible to match the answers you have added to the table above to the output of the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079df60-2915-46f5-af6b-18c71ad308db",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_facts(df_cars_facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ad302-03be-4617-b847-bad249a0aebe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371c454-ec2e-466f-beb5-175e2b9315b4",
   "metadata": {},
   "source": [
    "## 2 Clustering (25 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78616ba1-7794-4657-b555-87ede933041f",
   "metadata": {},
   "source": [
    "### 2.1 Implementing DBSCAN for Noise Detection (8 Points)\n",
    "\n",
    "In the lecture, we covered the original algorithm of DBSCAN, which you can also find on [Wikipedia](https://en.wikipedia.org/wiki/DBSCAN). While not difficult to implement, it takes quite a couple of lines of codes to do so. For this assignment, however, we are only interested in the points of a dataset that DBSCAN considers noise (as illustrated below; the red dots in the next plot). This includes that we do not have to care about\n",
    "\n",
    "* how many clusters there are (the plot below hints at 3 clusters but it does not matter) *and*\n",
    "* which non-noise points (the grey dots in the plot below) belong to which cluster\n",
    "\n",
    "**Your task is to implement a modified/simplified version of DBSCAN to find all noise points in a dataset!** The skeleton of method `get_noise_dbscan()` you need to complete is found in the file `A2.py` (before the appropriate renaming). The method takes data matrix `X` as well as the two basic parameters `eps` and `min_samples` as input parameters; we use the same naming as scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  The output should be 2 lists of indices: (a) one containing the indices of all *core points* and (b) one containing the indices of all *noise points* in input dataset X.\n",
    "\n",
    "**Important:**\n",
    "* We only split this task into 2.1 a) and 2.1 b) to have intermediate results you can check for correctness (and potentially to better allow for partial marking). Our reference solutions first finds all core points and uses this information to find all noise points; hence the 2 separate code blocks for you to complete.\n",
    "* However, if you have a better/faster/shorter/cooler/etc. solution, you are more than welcome to implement it and ignore the intermediate result of finding all core points. Only the result from 2.1 b) is important. This also means that you can ignore 2.1 a) and still get full marks if you correctly identify all noise points.\n",
    "* If you have an alternative solution, please make sure that the method still returns the 2 output parameters `(core_point_indices, noise_point_indices)`. If you do not need to explicitly identify the core points, you can simply return `None` for `core_point_indices`.\n",
    "* You can import any method `numpy`, `scipy`, `sklearn`, or `pandas` has to offer -- except for any ready-made implementation of DBSCAN, of course :). Please add any imports to the code cell at the top with the other imports. Hint: We already imported [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) for you.\n",
    "\n",
    "We will benchmark your implementation as part of our Little Competitions to see whose solution is the fastest.\n",
    "\n",
    "#### Dataset Preparation (nothing for you to do here; just run the following code cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7bd4b-db2f-4c7e-baea-34fad263a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dbscan_toy = pd.read_csv('data/a1-dbscan-toy-dataset.txt', header=None, sep=' ').to_numpy()\n",
    "\n",
    "print('The shape of X_dbscan_toy is {}'.format(X_dbscan_toy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81f8a0-7c40-423f-a788-7158c5493414",
   "metadata": {},
   "source": [
    "Now we can run scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on this dataset. Here we use `eps=0.1` and `min_samples=10` as values for the two main input parameters for DBSCAN that specify the minimum \"density\" of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8614a-649e-49ec-a923-c9787b160edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_clustering = DBSCAN(eps=0.1, min_samples=10).fit(X_dbscan_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f17f2-103f-482f-bb5a-3f476bf07ddb",
   "metadata": {},
   "source": [
    "The points that are noise points are labeled with `-1`, while all points belonging to clusters are labeled with `0`, `1`, `2`, etc. So we can easily find the indices of all the points labeled as noise as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b300653-0383-4636-9826-e1f0ccc88a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_point_indices = np.argwhere(dbscan_clustering.labels_ >= 0).squeeze()\n",
    "noise_point_indices = np.argwhere(dbscan_clustering.labels_ < 0).squeeze()\n",
    "\n",
    "print('The indices of the points labeled as noise are: {}'.format(noise_point_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab64f293-62be-42bd-87e3-581b171ff734",
   "metadata": {},
   "source": [
    "Of course, we can also plot the results. Note that the figure below only highlights the points labeled as noise as red triangles; all points belonging to *some* clusters are in grey points (note that we do not care to which exact cluster these points belong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834c622-5141-4c3e-9270-659a37fe9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_dbscan_toy[cluster_point_indices,0], X_dbscan_toy[cluster_point_indices,1], c='grey')\n",
    "plt.scatter(X_dbscan_toy[noise_point_indices,0], X_dbscan_toy[noise_point_indices,1], c='red', marker='^', s=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba889e5c-9371-451e-b90a-4ed2a1c2cce4",
   "metadata": {},
   "source": [
    "Summing up, the red dots in the plots we define as noise or outliers as they are very dissimilar to the other data points. In practice, we would likely remove those noise points, treat them separately, or maybe perform additional preprocessing steps to potentially \"denoise\" the dataset. However, the steps of choice generally depend heavily on the exact data mining task. Here, we focus on the identification of noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a99991-4c2c-4d63-bf5e-f0ac4063167f",
   "metadata": {},
   "source": [
    "#### 2.1 a) Compute Core Points (4 Points)\n",
    "\n",
    "As mentioned above, our reference solution first computes all core points. If you follow this approach, complete the respective part in the code of method `get_noise_dbscan()`. Some hints:\n",
    "* Recall that we do not care to which cluster a core point belongs to, only that it is a core point in *some* cluster\n",
    "* Have a look at method [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html); it might make your life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479dc9b0-e858-4714-a3d9-f19f65d149f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_core_point_indices, _ = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of core points: {}\\n'.format(len(my_core_point_indices)))\n",
    "print('The first 25 indices of the points labeled as core points:\\n{}'.format(sorted(my_core_point_indices)[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badb8159-815f-4e1f-8162-73b753d7a492",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "    \n",
    "```\n",
    "Total number of core points: 50\n",
    "\n",
    "The first 25 indices of the points labeled as core points:\n",
    "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n",
    "```\n",
    "\n",
    "Note that `0`, `4`, and `27` should be missing from this list since [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) told us that these points are noise. Of course, also the border points are missing here, but [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) does not return those explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b21a9-547e-44d8-a171-cd4be018841a",
   "metadata": {},
   "source": [
    "#### 2.1 b) Compute Noise Points (4 Points)\n",
    "\n",
    "Knowing the core points is useful but only an intermediate step. Now it is time to complete the method `get_noise_dbscan()` to compute the indices of all noise points in `X`. Again, our reference solution uses `core_point_indices` to accomplish this. If your implementation does not require the information about core points but returns the correct `noise_point_indices` then this is perfectly fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95080c60-a72b-456a-a774-13cdee689e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, my_noise_point_indices = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of noise points: {}\\n'.format(len(my_noise_point_indices)))\n",
    "print('The indices of all points labeled as noise points:\\n{}'.format(sorted(my_noise_point_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca898f-e675-4cba-adfc-c465873eba20",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "\n",
    "```\n",
    "Total number of noise points: 10\n",
    "\n",
    "The indices of all points labeled as noise points:\n",
    "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n",
    "```\n",
    "\n",
    "Since we used the same values for `eps` and `min_samples`, this result should match the output we saw earlier when we used scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over the toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1293f3-dfd4-43c8-af6a-4d4502508e49",
   "metadata": {},
   "source": [
    "### 2.2 Questions about Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355668ed-1dc0-4e72-bdfd-2af076eff91f",
   "metadata": {},
   "source": [
    "#### 2.2 a) Interpreting Dendrograms for Hierarchical Clusterings (6 Points)\n",
    "\n",
    "We saw in the lecture that dendrograms are a meaningful way to visualize the hierarchical relationships between the data points with respect to the clustering using AGNES (or any other hierarchical clustering technique). Properly interpreting is important to get a correct understanding of the underlying data.\n",
    "\n",
    "Below are the plots of 6 different datasets labeled A-F. Each dataset contains 30 data points, each with two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb68e26-1cd0-4802-aab6-03b24fe9e6b0",
   "metadata": {},
   "source": [
    "<img src=\"images/a1-agnes-data-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811bdb8c-69d7-4741-9a6f-052ac1f58b7f",
   "metadata": {},
   "source": [
    "Below are 6 dendrograms labeled 1-6. These dendograms show the clustering using **AGNES with Single Linkage** for the 6 datasets above, but in a random order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59ce51-63b0-4f07-ac22-ff9e951c7af5",
   "metadata": {},
   "source": [
    "<img src=\"images/a1-agnes-dendrogram-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b987ff-755b-479a-bd8f-6a5856d8d877",
   "metadata": {},
   "source": [
    "**Find the correct combinations of datasets and dendrograms** -- that is, find for each dataset the dendrogram that visualizes the clustering using AGNES with Single Linkage! Give a brief explanation for each decision! Complete the table below!\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b377ab0-8b46-44a8-a809-a615b8edab6a",
   "metadata": {},
   "source": [
    "| Dataset | Dendrogram | Brief Explanation |\n",
    "| ---  | ---   | ---                  |\n",
    "| **A**    | ??? | ??? |\n",
    "| **B**    | ??? | ??? |\n",
    "| **C**    | ??? | ??? |\n",
    "| **D**    | ??? | ??? |\n",
    "| **E**    | ??? | ??? |\n",
    "| **F**    | ??? | ??? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198a06d-514b-40a8-a20c-4bab9ce07125",
   "metadata": {},
   "source": [
    "#### 2.2 b) Comparing the Results of Different Clustering Algorithms (6 Points)\n",
    "\n",
    "The figure belows shows the 6 different clusterings A-F, each computed over a dataset of 8 unique data points $x_1 x_2, ..., x_8$. The datasets are independent from each other for the 6 clusterings. Each clustering contains 3 clusters are represented by the table. A `1` in the result table indicates that the corresponding data point is part of the corresponding cluster. For example, in Clustering A, the `1` in the bottom-left cell indicates that data point $x_8$ is part of Cluster $C_1$.\n",
    "\n",
    "**Addtional constraints:**\n",
    "\n",
    "* For K-Means and DBSCAN, the 3 cluster $C_1$, $C_2$, and $C_3$ are the **only** clusters; for AGNES you can assume there might be **more** clusters in the hierarchy\n",
    "* For DBSCAN, the input parameter for the minimum number of neighboring points is  $MinPts \\geq 2$\n",
    "\n",
    "<img src=\"images/a1-clustering-comparison.png\">\n",
    "\n",
    "**For each clustering, decide which algorithm (K-Means, DBSCAN, AGNES) can have produced the clustering!** Use the table below for the answer. If an algorithm could have produced a clustering, just write *OK* in the respective cell of the table. If an algorithm could not have produced a clustering, enter a brief explanation into the respective table cell.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae7dac-e450-4a85-a197-c709bb6ba6ea",
   "metadata": {},
   "source": [
    "|  | K-Means | DBSCAN       | AGNES |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| **Clustering A**  | ??? | ??? | \n",
    "| **Clustering B**  | ??? | ??? |\n",
    "| **Clustering C**  | ??? | ??? |\n",
    "| **Clustering D**  | ??? | ??? |\n",
    "| **Clustering E**  | ??? | ??? |\n",
    "| **Clustering F**  | ??? | ??? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5c7a8-d575-4557-8fab-1e12ece90e5e",
   "metadata": {},
   "source": [
    "#### 2.2 c) Short Essay Questions (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca25144-b734-422d-96eb-4ca3a3742bf5",
   "metadata": {},
   "source": [
    "Assume you have a dataset `X`, run DBSCAN, and get a clustering that contains a set of clusters and some noise points (there's no need to be more precise; it's only important that you don't get just noise). Let's also assume you create a new dataset `X_new` simply by shuffling `X` (i.e., randomly change the order of data points in the dataset); no other changes. Now you run DBSCAN with the *same* parameters as before over `X_new` and get a different clustering, i.e., most of the clusters are not exactly the same as before.\n",
    "\n",
    "**What does this information tell about the dataset and clustering? (3 Points)** This may include a brief discussion how changing the parameters of DBSCAN will likely affect the results.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344584c-2312-4196-a2b6-bca4cb17c14b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b07b87ed-c23e-4f06-b25f-0cd04849937b",
   "metadata": {},
   "source": [
    "In the lecture, we used the Euclidean Distance between data points in centroids to perform the Lloyd's algorithm. However, there are other distance metrics such as the [Manhatten Distance](https://en.wikipedia.org/wiki/Taxicab_geometry) or the [Minkowski Distance](https://en.wikipedia.org/wiki/Minkowski_distance). Can we use those or similar metrics instead of the Euclidean Distance? Give a **brief** justification for your answer!\n",
    "\n",
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f1833-b931-4bfc-b4c8-63f3d51f8a0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057530f-3c82-47b9-b6c8-09f70c1fe405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
